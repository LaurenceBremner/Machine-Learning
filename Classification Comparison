#Load Libaries and datasets
import pandas as pd
import numpy as np
from pandas.plotting import scatter_matrix

#Plotting
import matplotlib.pyplot as plt

#Don't need proprocessing as the built in datasets are already normalised

#Model Selection
from sklearn.model_selection import KFold
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import train_test_split

#Get confusion matrix and classification report
from sklearn.metrics import classification_report
from sklearn.metrics import confusion matrix
from sklearn.metrics import accuracy_score
from sklearn.metrics import scatter_matrix

#Classifiers
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import AdaBoostClassifier
from sklearn.tree import DecisionTreeClasifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.disciminant_analysis import LinearDiscriminantAnalysis

#Load data and create dataframs
#from sklearn.datasets import load_iris

#Load Data
#iris = load_iris()

#Load DataSet
feature_names = ['sepal_length','sepal_width','petal_length','petal_width','flower_type']
#Thus, there are four feature variables and one class variable

#Create dataframe
df = pd.read_csv('iris.data.csv', names=feature_names)

#Visualise Data (first 5 observations)
print(df.head())
print(df.shape)
print(df.describe())
print(df.groupby('flower_type').size())

#Create labelled histograms
df.hist()
plt.ylabel('Frequency')
plt.xlabel('Length')
plt.show()

#Create multivariate plots for variable correlations/interactions
scatter_matrix(df)

#Use train test split to split into training, and validation dataset
X = df.values[:,0:4]
Y = df.values[:,4]

test_size = 0.33
seed = 2
X_train, X_test, Y_train, Y_test = train_test_split(X,Y,test_size=test_size, random_state=seed)

#Classification Comparison
names = ["Random Forest ", "AdaBoost ", "Decision Tree", "Nearest Neighbors", "Linear SVM SVC", "LDA"]
classifiers = [RandomForestClassifier(), AdaBoostClassifier(), DecisionTreeClassifier(), KNeighborsClassifier(), SVC(), LinearDiscriminantAnalysis()]

#Define scoring system
scoring = 'accuracy'

#Create array of cross validation results
scores = []
#Create array of the classifier names
model_names = []

averages = []

for name,classifier in list(zip(names,classifiers)):
  kfold = KFold(n_splits=10,random_state=seed)
  cross_val_results=cross_val_score(classifier,X_train,Y_train,cv=kfold,scoring=scoring)
  model_names.append(name)
  scores.append(cross_val_results)
  print("The %s model had a mean accuracy of %f, and a standard deviation of %f" % (name, cross_val_results.mean(), cross_val_results.std()))
  averages.append(cross_val_results.mean())
 
#Get index of top performing model. We will use this later to test the top 2 classifiers against each other
index = np.argsort(-np.array(averages))[:2]

best_model = model_names[int(index)]
mean_score = scores[index].mean()
standard_deviation = scores[index].std()

print("The best model was %s, with a mean accuracy of %f, and a standard deviation of (%f)" % (best_algo, mean_score, standard_deviation))

#Create a plot of the algorithms

#As well as accuracy we look at recall and f1 score to give a fairer idea of how good the model is
#Get accuracy, recall, and f1 score

#Test model on validation dataset
best.fit(X_train,Y_train)
output = best.predict(X_test)
print(classification_report(Y_test, output))
print(accuracy_score(Y_test, output))
print(confusion_matrix(Y_test, output))

